{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g30zYblniYm1"
   },
   "source": [
    "# CNN with ResNet for CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "5qoJVI0RCyaH"
   },
   "outputs": [],
   "source": [
    "# Imports for pytorch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zmwd_kzNT-Cq",
    "outputId": "b9442cef-8177-4fa5-fa28-0f69440f699b"
   },
   "outputs": [],
   "source": [
    "# Creating the datasets and applying transformations\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.RandomCrop(32, padding=4),\n",
    "    torchvision.transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    torchvision.transforms.RandomRotation(15),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "val_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "training_data = torchvision.datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "testing_data = torchvision.datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=val_transform,\n",
    ")\n",
    "\n",
    "# setting a training/validation split of 80-20\n",
    "train_size = .8*len(training_data)\n",
    "val_size = len(training_data) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(training_data, [int(train_size), int(val_size)])\n",
    "\n",
    "train_dataset.dataset.transform = transform\n",
    "val_dataset.dataset.transform = val_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xY7rDFcg3og6"
   },
   "source": [
    "Visualizing loaded CIFAR10 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "6PNE-4LT3lx8",
    "outputId": "63194a8c-0eec-45a7-b271-f211856ecb5d"
   },
   "outputs": [],
   "source": [
    "images = [training_data[i][0] for i in range(9)]\n",
    "plt.imshow(torchvision.utils.make_grid(torch.stack(images), nrow=3, padding=5).numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FfwQKKylP7L"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I4DyQNjKjlol",
    "outputId": "599c8411-109d-4af9-beac-31b185b8a177"
   },
   "outputs": [],
   "source": [
    "# Initialize the weights of the model using Xavier initialization\n",
    "def xavier_init(model):\n",
    "    if isinstance(model, nn.Conv2d) or isinstance(model, nn.Linear):\n",
    "        nn.init.xavier_normal_(model.weight)  # Xavier initialization for weights\n",
    "        if model.bias is not None:\n",
    "            nn.init.zeros_(model.bias)  # Initialize bias to zero\n",
    "\n",
    "# Define the ResNet model\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        conv1_out = 64\n",
    "        res1_out = 64\n",
    "        res2_out = 128\n",
    "        res3_out = 256\n",
    "\n",
    "        # Initial convolution layer with batch normalization and max pooling\n",
    "        self.dropout = nn.Dropout(p=0.6)\n",
    "        self.conv1 = nn.Conv2d(3, conv1_out, 3, 1, 1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(conv1_out)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Define three residual blocks with increasing number of filters\n",
    "        self.myRes1 = nn.Sequential(\n",
    "            nn.Conv2d(conv1_out, res1_out, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(res1_out),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(res1_out, res1_out, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(res1_out),\n",
    "        )\n",
    "        self.myRes2 = nn.Sequential(\n",
    "            nn.Conv2d(res1_out, res2_out, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(res2_out),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(res2_out, res2_out, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(res2_out),\n",
    "        )\n",
    "        self.myRes3 = nn.Sequential(\n",
    "            nn.Conv2d(res2_out, res3_out, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(res3_out),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(res3_out, res3_out, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(res3_out),\n",
    "        )\n",
    "\n",
    "        # Downsampling layers for skip connections in residual blocks\n",
    "        self.oneByOne_downsample1 = nn.Conv2d(conv1_out, res1_out, 1, 1, bias=False)\n",
    "        self.oneByOne_downsample2 = nn.Conv2d(res1_out, res2_out, 1, 1, bias=False)\n",
    "        self.oneByOne_downsample3 = nn.Conv2d(res2_out, res3_out, 1, 1, bias=False)\n",
    "\n",
    "        # Average pooling and fully connected layer for classification\n",
    "        self.averagePool = torch.nn.AvgPool2d(2)\n",
    "        self.fullyConnected = nn.Linear(res3_out, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial convolution + batch norm + ReLU + max pooling\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn(out)\n",
    "        out = torch.nn.functional.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "\n",
    "        # Residual block 1 with skip connection\n",
    "        skip_x = out\n",
    "        out = self.myRes1(out)\n",
    "        ds_skip = self.oneByOne_downsample1(skip_x)\n",
    "        out = torch.nn.functional.relu(out + ds_skip)\n",
    "        out = self.maxpool(out)\n",
    "\n",
    "        # Residual block 2 with skip connection\n",
    "        skip_x = out\n",
    "        out = self.myRes2(out)\n",
    "        ds_skip = self.oneByOne_downsample2(skip_x)\n",
    "        out = torch.nn.functional.relu(out + ds_skip)\n",
    "        out = self.maxpool(out)\n",
    "\n",
    "        # Residual block 3 with skip connection\n",
    "        skip_x = out\n",
    "        out = self.myRes3(out)\n",
    "        ds_skip = self.oneByOne_downsample3(skip_x)\n",
    "        out = torch.nn.functional.relu(out + ds_skip)\n",
    "        out = self.maxpool(out)\n",
    "\n",
    "        # Global average pooling, flattening, dropout, and fully connected layer\n",
    "        out = self.averagePool(out)\n",
    "        out = torch.flatten(out, start_dim=1)\n",
    "        out = self.dropout(out)\n",
    "        y = self.fullyConnected(out)\n",
    "        return y\n",
    "\n",
    "# Train the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device\", device)\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize model and apply Xavier initialization\n",
    "model = ResNet().to(device)\n",
    "xavier_init(model)\n",
    "\n",
    "# Define optimizer, learning rate scheduler, and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Create data loaders for training, validation, and testing\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "test_dataloader = torch.utils.data.DataLoader(testing_data, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# Lists for plotting losses\n",
    "plt_VL = []  # Validation loss\n",
    "plt_TL = []  # Training loss\n",
    "\n",
    "model.train()  # Put model in training mode\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    training_losses = []\n",
    "    train_correct = 0\n",
    "    for x, y in tqdm.tqdm(train_dataloader, unit=\"batch\"):\n",
    "        x, y = x.float().to(device), y.long().to(device)\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        pred = model(x)\n",
    "        train_correct += (torch.argmax(pred, dim=1) == y).sum().item()  # Count correct predictions\n",
    "        loss = criterion(pred, y)\n",
    "        loss.backward()  # Compute gradients\n",
    "        optimizer.step()  # Update weights\n",
    "        training_losses.append(loss.item())\n",
    "\n",
    "    # Validation phase\n",
    "    val_correct = 0\n",
    "    val_losses = []\n",
    "    for x, y in val_dataloader:\n",
    "        x, y = x.float().to(device), y.long().to(device)\n",
    "        pred = model(x)\n",
    "        loss = criterion(pred, y)\n",
    "        val_correct += (torch.argmax(pred, dim=1) == y).sum().item()  # Count correct predictions\n",
    "        val_losses.append(loss.item())\n",
    "\n",
    "    # Compute and display accuracies and losses for this epoch\n",
    "    train_accuracy = (train_correct / len(train_dataset)) * 100\n",
    "    val_accuracy = (val_correct / len(val_dataset)) * 100\n",
    "    print(f\"Epoch {epoch + 1}, val accuracy: {val_accuracy:.2f}%, train accuracy: {train_accuracy:.2f}%, val loss: {np.mean(val_losses):.4f}, training loss: {np.mean(training_losses):.4f}\")\n",
    "    plt_VL.append(np.mean(val_losses))\n",
    "    plt_TL.append(np.mean(training_losses))\n",
    "    model.train()  # Put model back in training mode\n",
    "    scheduler.step()  # Adjust learning rate\n",
    "\n",
    "# Compute final test accuracy\n",
    "with torch.no_grad():\n",
    "    model.eval()  # Put model in evaluation mode\n",
    "    num_correct = 0\n",
    "    for x, y in test_dataloader:\n",
    "        x, y = x.float().to(device), y.long().to(device)\n",
    "        pred = model(x)\n",
    "        num_correct += (torch.argmax(pred, dim=1) == y).sum().item()\n",
    "    print(\"Final Accuracy:\", num_correct / len(testing_data) * 100)\n",
    "    model.train()  # Put model back in training mode\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9LMY1UbsL6-"
   },
   "source": [
    "Plot training and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "qDgQ2JcJr970",
    "outputId": "e1a3343c-3b40-49d5-9b57-e5b16226053c"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = np.arange(1,11,1)\n",
    "plt.plot(x, plt_VL, label=\"validation loss\")\n",
    "plt.plot(x, plt_TL, label=\"training loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPcuaeJFsG2B"
   },
   "source": [
    "Prepare CIFAR10 test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CLSfYTCJqvBB",
    "outputId": "01e3689b-408e-4641-f701-28536d7437d5"
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the same transformations as used during training and validation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to match training preprocessing\n",
    "])\n",
    "\n",
    "# Load the CIFAR-10 test set\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',  # Directory to store the dataset\n",
    "    train=False,    # This ensures we get the test set\n",
    "    download=True,  # Download if not already available\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Create a DataLoader for the test set\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmnMYE8Qr_I5"
   },
   "source": [
    "Calculate test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GOp7pzZIpLQa",
    "outputId": "d8c11335-f703-4ae6-c7e4-0aa067a25aa8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 82.08%\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'model' and 'device' are already defined\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total * 100\n",
    "print(f'Test Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxOtkx6rs0HH"
   },
   "source": [
    "Display random image from test set alongside its true and predicted class values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "JunM456nrLCA",
    "outputId": "18e88c4f-fa78-4555-dd2e-8cb195f6b3db"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "def show_random_prediction():\n",
    "    # Randomly select an index from the test dataset\n",
    "    index = random.randint(0, len(test_dataset) - 1)\n",
    "    img, label = test_dataset[index]\n",
    "\n",
    "    # Convert the image back to [0, 1] range for display\n",
    "    img_display = img * 0.5 + 0.5  # Undo normalization\n",
    "    img_display = img_display.permute(1, 2, 0).numpy()  # Change to HWC format for plotting\n",
    "\n",
    "    # Get the model's prediction\n",
    "    img = img.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "    with torch.no_grad():\n",
    "        output = model(img)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "\n",
    "    predicted_label = classes[predicted.item()]\n",
    "    true_label = classes[label]\n",
    "\n",
    "    # Display the image and classification\n",
    "    plt.imshow(img_display)\n",
    "    plt.title(f'Prediction: {predicted_label} | True Label: {true_label}')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to show a random prediction\n",
    "show_random_prediction()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torch_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
